# Vietnamese Custom SentencePiece Tokenizer

Bá»™ cÃ´ng cá»¥ Ä‘á»ƒ xÃ¢y dá»±ng tokenizer SentencePiece tÃ¹y chá»‰nh cho tiáº¿ng Viá»‡t tá»« raw data cá»§a báº¡n.

## ğŸ¯ TÃ­nh nÄƒng

- **Thu tháº­p raw text** tá»« cÃ¡c dataset Wiki, VNews, IWSLT15
- **Train SentencePiece tokenizer** vá»›i nhiá»u kÃ­ch thÆ°á»›c vocab (16K, 32K, 50K)
- **Wrapper class** tÆ°Æ¡ng thÃ­ch vá»›i Transformers library
- **Xá»­ lÃ½ data** tá»± Ä‘á»™ng vá»›i custom tokenizer
- **Pipeline hoÃ n chá»‰nh** tá»« raw data Ä‘áº¿n tokenized data

## ğŸ“‹ YÃªu cáº§u

```bash
pip install -r requirements.txt
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Chuáº©n bá»‹ dá»¯ liá»‡u

Äáº£m báº£o báº¡n cÃ³ dá»¯ liá»‡u trong cáº¥u trÃºc sau:

```
data/raw/
â”œâ”€â”€ Wiki/
â”‚   â”œâ”€â”€ train.tsv
â”‚   â”œâ”€â”€ valid.tsv
â”‚   â””â”€â”€ test.tsv
â”œâ”€â”€ VNews/
â”‚   â”œâ”€â”€ train.tsv
â”‚   â”œâ”€â”€ valid.tsv
â”‚   â””â”€â”€ test.tsv
â””â”€â”€ iwslt15/
    â”œâ”€â”€ train.en
    â”œâ”€â”€ train.vi
    â”œâ”€â”€ tst2012.en
    â”œâ”€â”€ tst2012.vi
    â”œâ”€â”€ tst2013.en
    â””â”€â”€ tst2013.vi
```

### 2. Cháº¡y pipeline hoÃ n chá»‰nh

```bash
python build_tokenizer/build_complete_pipeline.py
```

Hoáº·c cháº¡y tá»«ng bÆ°á»›c:

#### BÆ°á»›c 1: Thu tháº­p raw text
```bash
python build_tokenizer/collect_raw_text.py
```

#### BÆ°á»›c 2: Train tokenizer
```bash
python build_tokenizer/train_sentencepiece.py
```

#### BÆ°á»›c 3: Test tokenizer
```bash
python build_tokenizer/custom_tokenizer.py
```

#### BÆ°á»›c 4: Process data
```bash
python build_tokenizer/process_data_with_custom_tokenizer.py
```

## ğŸ“Š Káº¿t quáº£

Sau khi cháº¡y pipeline, báº¡n sáº½ cÃ³:

```
build_tokenizer/
â”œâ”€â”€ raw_text_for_tokenizer.txt      # Raw text Ä‘á»ƒ train tokenizer
â”œâ”€â”€ vietnamese_tokenizer_16000.model # Tokenizer 16K vocab
â”œâ”€â”€ vietnamese_tokenizer_32000.model # Tokenizer 32K vocab
â”œâ”€â”€ vietnamese_tokenizer_50000.model # Tokenizer 50K vocab
â””â”€â”€ saved_tokenizer/                 # Tokenizer Ä‘Ã£ save

data/custom_tokenized/
â”œâ”€â”€ tokenizer/                       # Tokenizer Ä‘á»ƒ dÃ¹ng trong training
â”œâ”€â”€ summarization_train/             # Summarization data
â”œâ”€â”€ summarization_valid/
â”œâ”€â”€ summarization_test/
â”œâ”€â”€ translation_train/               # Translation data
â”œâ”€â”€ translation_valid/
â”œâ”€â”€ translation_test/
â””â”€â”€ processing_info.json             # Metadata
```

## ğŸ’» Sá»­ dá»¥ng trong code

### Load tokenizer

```python
from build_tokenizer.custom_tokenizer import CustomSentencePieceTokenizer

# Load tokenizer
tokenizer = CustomSentencePieceTokenizer.from_pretrained('data/custom_tokenized/tokenizer')

# Hoáº·c load tá»« model file trá»±c tiáº¿p
tokenizer = CustomSentencePieceTokenizer('build_tokenizer/vietnamese_tokenizer_32000.model')
```

### Tokenize text

```python
# Single text
text = "Xin chÃ o, tÃ´i lÃ  AI assistant."
result = tokenizer(text, max_length=50, padding=True, return_tensors="pt")
print(result['input_ids'])
print(result['attention_mask'])

# Batch texts
texts = ["CÃ¢u 1", "CÃ¢u 2", "CÃ¢u 3"]
batch_result = tokenizer(texts, max_length=50, padding=True, return_tensors="pt")
print(batch_result['input_ids'].shape)  # [3, 50]
```

### Decode tokens

```python
# Decode single sequence
token_ids = [2, 1234, 5678, 3]  # [BOS, tokens..., EOS]
decoded = tokenizer.decode(token_ids)
print(decoded)

# Batch decode
sequences = [[2, 1234, 3], [2, 5678, 9012, 3]]
decoded_texts = tokenizer.batch_decode(sequences)
print(decoded_texts)
```

### Load processed data

```python
from datasets import load_from_disk

# Load summarization data
train_dataset = load_from_disk('data/custom_tokenized/summarization_train')
valid_dataset = load_from_disk('data/custom_tokenized/summarization_valid')

# Load translation data
mt_train = load_from_disk('data/custom_tokenized/translation_train')

print(f"Train size: {len(train_dataset)}")
print(f"Sample: {train_dataset[0]}")
```

### TÃ­ch há»£p vá»›i Transformers

```python
from transformers import DataCollatorForSeq2Seq

# Táº¡o data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    return_tensors="pt",
    padding=True
)

# Sá»­ dá»¥ng trong DataLoader
from torch.utils.data import DataLoader

dataloader = DataLoader(
    train_dataset,
    batch_size=4,
    collate_fn=data_collator
)

for batch in dataloader:
    print(batch.keys())  # ['input_ids', 'attention_mask', 'labels']
    break
```

## âš™ï¸ TÃ¹y chá»‰nh

### Thay Ä‘á»•i vocab size

Sá»­a file `train_sentencepiece.py`:

```python
vocab_sizes = [8000, 16000, 24000, 32000]  # ThÃªm kÃ­ch thÆ°á»›c khÃ¡c
```

### Thay Ä‘á»•i model type

```python
model_type='unigram'  # Thay vÃ¬ 'bpe'
```

### Thay Ä‘á»•i character coverage

```python
character_coverage=0.999  # TÄƒng Ä‘á»ƒ cover nhiá»u kÃ½ tá»± hÆ¡n
```

### ThÃªm special tokens

```python
user_defined_symbols="<mask>,<sep>,<cls>"  # ThÃªm tokens tÃ¹y chá»‰nh
```

## ğŸ”§ Troubleshooting

### Lá»—i "Model file not found"
```bash
# Kiá»ƒm tra file cÃ³ tá»“n táº¡i khÃ´ng
ls -la build_tokenizer/vietnamese_tokenizer_*.model
```

### Lá»—i memory khi train tokenizer
```python
# Giáº£m max_sentence_length
max_sentence_length=2048  # Thay vÃ¬ 4192
```

### Lá»—i encoding
```python
# Äáº£m báº£o file Ä‘Æ°á»£c má»Ÿ vá»›i encoding UTF-8
with open(file_path, 'r', encoding='utf-8') as f:
```

## ğŸ“ˆ So sÃ¡nh vocab sizes

| Vocab Size | Model Size | Speed | Coverage | Use Case |
|------------|------------|--------|----------|----------|
| 16K        | Nhá»        | Nhanh  | Tháº¥p     | Testing, small models |
| 32K        | Vá»«a        | Vá»«a    | Tá»‘t      | **Recommended** |
| 50K        | Lá»›n        | Cháº­m   | Cao      | Large models, best quality |

## ğŸ¯ Best Practices

1. **Chá»n vocab size 32K** cho háº§u háº¿t cÃ¡c á»©ng dá»¥ng
2. **Sá»­ dá»¥ng BPE** cho tiáº¿ng Viá»‡t
3. **Character coverage 0.9995** cho tiáº¿ng Viá»‡t
4. **Test tokenizer** vá»›i nhiá»u loáº¡i text khÃ¡c nhau
5. **Save tokenizer config** Ä‘á»ƒ reproduce Ä‘Æ°á»£c káº¿t quáº£

## ğŸ“ Há»— trá»£

Náº¿u gáº·p váº¥n Ä‘á», hÃ£y kiá»ƒm tra:

1. Cáº¥u trÃºc thÆ° má»¥c data Ä‘Ãºng chÆ°a
2. Dependencies Ä‘Ã£ install Ä‘á»§ chÆ°a
3. File permissions cÃ³ OK khÃ´ng
4. Disk space Ä‘á»§ khÃ´ng (cáº§n ~2GB cho pipeline hoÃ n chá»‰nh) 